data:
  # CSV for path and metadata to training examples.
  dataset:
    max_num_res: 128
    min_num_res: 0
    subset: null
    samples_per_eval_length: 2
    num_eval_lengths: 4
    min_eval_length: 500
    csv_path: ./preprocessed/metadata.csv
  loader:
    num_workers: 4
    prefetch_factor: 10
  sampler:
    max_batch_size: 128
    max_num_res_squared: 1_000_000
    batch_repeats: 1
    num_batches: null

model:
  architecture: framediff

  # Architecture based on GENIE
  flower:
    node_embed_size: 128
    edge_embed_size: 128
    node_features:
      c_s: ${model.flower.node_embed_size}
      c_pos_emb: 128
      c_timestep_emb: 128
      max_num_res: 2000
      timestep_int: 1000
    edge_features:
      c_s: ${model.flower.node_embed_size}
      c_p: ${model.flower.edge_embed_size}
      relpos_k: 32
    pair_network:
      c_p: ${model.flower.edge_embed_size}
      include_mul_update: True
      inlude_tri_att: False
      c_hidden_mul: 128
      pair_transition_n: 4
      n_pair_transform_layer: 5
    structure_network:
      n_structure_layer: 5
      dropout: 0.0
      n_structure_transition_layer: 1
      structure_transition_dropout: 0.0
      ipa:
        c_s: ${model.flower.node_embed_size}
        c_z: ${model.flower.edge_embed_size}
        c_hidden: 16
        no_heads: 12
        no_qk_points: 4
        no_v_points: 8

  framediff:
    t_embed_size: 64
    index_embed_size: 64
    node_embed_size: 256
    edge_embed_size: 128
    dropout: 0.0
    ipa:
      c_s: ${model.framediff.node_embed_size}
      c_z: ${model.framediff.edge_embed_size}
      c_hidden: 128
      c_skip: 64
      no_heads: 8
      no_qk_points: 8
      no_v_points: 12
      seq_tfmr_num_heads: 4
      seq_tfmr_num_layers: 2
      num_blocks: 4

experiment:
  debug: False
  seed: 123
  num_devices: 2
  noise_trans: True
  noise_rots: True
  batch_ot:
    enabled: True
    noise_per_sample: 1
  training:
    loss: bb_atom_loss
    superimpose: all_atom  # c_alpha
  wandb:
    name: baseline
    project: se3-fm
    tags: ['baseline']
  optimizer:
    lr: 0.0001
  trainer:
    overfit_batches: 0
    min_epochs: 1 # prevents early stopping
    max_epochs: 1000
    accelerator: gpu
    log_every_n_steps: 1
    check_val_every_n_epoch: 1
    deterministic: False
    strategy: ddp
  sampling:
    num_timesteps: 100
    min_t: 1e-3
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}

