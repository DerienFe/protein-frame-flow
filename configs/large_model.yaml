defaults:
  - base
  - _self_

# data:
  # loader:
  #   num_workers: 10
  #   prefetch_factor: 25
  # sampler:
    # max_num_res_squared: 500_000
  #   max_batch_size: 128
  #   max_num_res_squared: 1_500_000
  #   use_batch_repeats: True

model:
  flow:
    ipa:
      num_blocks: 6

experiment:
  debug: False
  # so3_prior: uniform
  wandb:
    name: large_model

  # training:
  #   self_correcting: True

  # trainer:
  #   accumulate_grad_batches: 4
  #   check_val_every_n_epoch: 5

  # warm_start: ./ckpt/se3-fm/large_model/2023-09-30_08-30-01/last.ckpt
  # warm_start: ./ckpt/se3-fm/baseline/2023-09-28_19-24-06/last.ckpt
  # warm_start: ./ckpt/se3-fm/hybrid_loss/2023-09-24_10-55-20/last.ckpt
  # warm_start: ./ckpt/se3-fm/baseline_faster/2023-09-23_08-04-04/last.ckpt
  # warm_start: ./ckpt/se3-fm/self_correcting/2023-09-20_22-38-33/last.ckpt
  # warm_start: ./ckpt/se3-fm/self_correcting_faster/2023-09-23_08-26-01/last.ckpt
  # warm_start: ./ckpt/se3-fm/baseline_self_correct/2023-09-21_20-16-25/last.ckpt
  # warm_start: ./ckpt/se3-fm/no_time_embed/2023-09-22_12-22-37/last.ckpt

  # trainer:
  #   accumulate_grad_batches: 4
  #   check_val_every_n_epoch: 5
