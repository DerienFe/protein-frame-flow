defaults:
  - base
  - _self_

data:
  # loader:
  #   num_workers: 10
  #   prefetch_factor: 25
  sampler:
    # max_num_res_squared: 500_000
    max_batch_size: 128
    max_num_res_squared: 1_500_000
    # use_batch_repeats: True

# model:
#   flow:
#     ipa:
#       num_blocks: 6

experiment:
  debug: False
  # so3_prior: uniform
  wandb:
    name: warm_start_self_cond_1

  # training:
    # aux_loss_weight: 1.0
    # aux_loss_t_pass: 0.5
  #   self_correcting: True

  trainer:
    accumulate_grad_batches: 4
    check_val_every_n_epoch: 2

  warm_start: ./ckpt/se3-fm/warm_start_self_cond_1/2023-09-30_19-03-53/last.ckpt
  # warm_start: ./ckpt/se3-fm/warm_start_aux_loss/2023-09-30_19-24-08/last.ckpt
  # warm_start: ./ckpt/se3-fm/warm_start_batch_repeats/2023-09-30_15-52-15/last.ckpt
  # warm_start: ./ckpt/se3-fm/warm_start/2023-09-30_15-50-12/last.ckpt
  # warm_start: ./ckpt/se3-fm/warm_start/2023-09-30_15-50-12/last.ckpt
  # warm_start: ./ckpt/se3-fm/warm_start_self_condition/2023-09-29_14-34-20/last.ckpt
  # warm_start: ./ckpt/se3-fm/large_model/2023-09-30_08-30-01/last.ckpt
  # warm_start: ./ckpt/se3-fm/baseline/2023-09-28_19-24-06/last.ckpt
  # warm_start: ./ckpt/se3-fm/hybrid_loss/2023-09-24_10-55-20/last.ckpt
  # warm_start: ./ckpt/se3-fm/baseline_faster/2023-09-23_08-04-04/last.ckpt
  # warm_start: ./ckpt/se3-fm/self_correcting/2023-09-20_22-38-33/last.ckpt
  # warm_start: ./ckpt/se3-fm/self_correcting_faster/2023-09-23_08-26-01/last.ckpt
  # warm_start: ./ckpt/se3-fm/baseline_self_correct/2023-09-21_20-16-25/last.ckpt
  # warm_start: ./ckpt/se3-fm/no_time_embed/2023-09-22_12-22-37/last.ckpt

  # trainer:
  #   accumulate_grad_batches: 4
  #   check_val_every_n_epoch: 5
